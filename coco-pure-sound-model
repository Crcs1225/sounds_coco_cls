{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12948599,"sourceType":"datasetVersion","datasetId":8194363},{"sourceId":13064297,"sourceType":"datasetVersion","datasetId":8273337}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install torch torchaudio transformers datasets accelerate soundfile librosa audiomentations --upgrade\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If you're using conda, try:\n!conda install -c pytorch torch torchaudio -y\n!conda install -c conda-forge librosa -y\n!pip install transformers datasets accelerate soundfile audiomentations --upgrade","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport math\nimport json\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport soundfile as sf\nimport torch\nimport torchaudio\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchaudio.transforms import Resample\nfrom transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSR = 16000  # Wav2Vec2 expects 16kHz mono\nTARGET_SECONDS = 1.5\nTARGET_SAMPLES = int(SR * TARGET_SECONDS)\nDATA_DIR = Path(\"/kaggle/input/coco-sound/sounds dataset\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data and exploratory analysis","metadata":{}},{"cell_type":"code","source":"#Dataset structure loading\n# Expecting: DATA_DIR / class_a / *.wav, DATA_DIR / class_b / *.wav, DATA_DIR / class_c / *.wav\ndef list_audio_files(root: Path) -> pd.DataFrame:\n    rows = []\n    for cls in sorted([d for d in root.iterdir() if d.is_dir()]):\n        for wav in cls.glob(\"*.wav\"):\n            rows.append({\"path\": str(wav), \"label\": cls.name})\n    return pd.DataFrame(rows)\n\ndf = list_audio_files(DATA_DIR)\nprint(\"Total files:\", len(df))\ndf.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Class distribution\nplt.figure(figsize=(6,4))\nsns.countplot(y=\"label\", data=df, order=df[\"label\"].value_counts().index)\nplt.title(\"Class counts\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Duration statistics\ndef get_duration(path):\n    info = sf.info(path)\n    return info.duration\n\ndf[\"duration_s\"] = df[\"path\"].apply(get_duration)\ndf.groupby(\"label\")[\"duration_s\"].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.histplot(df[\"duration_s\"], bins=30, kde=True)\nplt.title(\"Audio duration distribution (seconds)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quick waveform visualization - CORRECTED VERSION\ndef plot_waveform(path, sr=SR, title=None):\n    x, file_sr = sf.read(path, always_2d=False)\n    if file_sr != sr:\n        x = librosa.resample(x, orig_sr=file_sr, target_sr=sr)  # Fixed line\n    plt.figure(figsize=(10,3))\n    plt.plot(x)\n    plt.title(title or Path(path).name)\n    plt.xlabel(\"Samples\")\n    plt.ylabel(\"Amplitude\")\n    plt.tight_layout()\n    plt.show()\n\nsample_paths = df.sample(3, random_state=42)[\"path\"].tolist()\nfor p in sample_paths:\n    plot_waveform(p, title=f\"Waveform: {Path(p).name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing and augmentation","metadata":{}},{"cell_type":"markdown","source":"- Resample: All audio to 16 kHz mono.\n- Normalize: Per-sample RMS normalization.\n- Crop/pad: Random 1-second segments for training; center crops for validation/test.\n- Augmentations (subtle, tap-safe):\n- Time shift: Small circular shift.\n- Background noise: Low-level Gaussian or mix-of dataset ambient fragments.\n- Random gain: Mild amplitude scaling.\n- Avoid strong pitch-shift/time-stretch; taps are brief transients whose cues can be distorted.\n","metadata":{}},{"cell_type":"code","source":"import torchaudio.transforms as T\nimport numpy as np\nimport soundfile as sf\nimport librosa\nfrom transformers import Wav2Vec2FeatureExtractor\n\n# Define all your functions FIRST\ndef load_audio(path, sr=SR):\n    x, file_sr = sf.read(path, always_2d=False)\n    if x.ndim > 1:  # stereo to mono\n        x = np.mean(x, axis=1)\n    if file_sr != sr:\n        x = librosa.resample(x, orig_sr=file_sr, target_sr=sr)\n    return x.astype(np.float32)\n\ndef rms_normalize(x, eps=1e-8):\n    rms = np.sqrt(np.mean(x**2))\n    if rms < eps:\n        return x\n    return x / rms\n\ndef crop_or_pad(x: np.ndarray, n_samples: int, mode=\"random\"):\n    if len(x) >= n_samples:\n        if mode == \"random\":\n            start = np.random.randint(0, len(x) - n_samples + 1)\n        else:\n            start = max(0, (len(x) - n_samples) // 2)\n        return x[start:start + n_samples]\n    pad_left = (n_samples - len(x)) // 2\n    pad_right = n_samples - len(x) - pad_left\n    return np.pad(x, (pad_left, pad_right), mode='constant')\n\ndef time_shift(x, max_shift=0.1):\n    shift = int(max_shift * len(x) * np.random.uniform(-1, 1))\n    if shift > 0:\n        return np.concatenate([x[shift:], np.zeros(shift)])\n    elif shift < 0:\n        return np.concatenate([np.zeros(-shift), x[:shift]])\n    return x\n\ndef add_gaussian_noise(x, min_amplitude=0.001, max_amplitude=0.01):\n    if np.random.random() < 0.3:\n        amplitude = np.random.uniform(min_amplitude, max_amplitude)\n        noise = np.random.normal(0, amplitude, x.shape)\n        return x + noise\n    return x\n\ndef gain_augmentation(x, min_gain_db=-3.0, max_gain_db=3.0):\n    if np.random.random() < 0.3:\n        gain_db = np.random.uniform(min_gain_db, max_gain_db)\n        gain_linear = 10 ** (gain_db / 20.0)\n        return x * gain_linear\n    return x\n\ndef train_augment(x):\n    x = time_shift(x, max_shift=0.1)\n    x = add_gaussian_noise(x, 0.001, 0.01)\n    x = gain_augmentation(x, -3.0, 3.0)\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Dataset class and split\nfrom sklearn.model_selection import StratifiedGroupKFold, train_test_split\n\nlabels = sorted(df[\"label\"].unique())\nlabel2id = {l:i for i,l in enumerate(labels)}\nid2label = {i:l for l,i in label2id.items()}\n\ndf[\"label_id\"] = df[\"label\"].map(label2id)\n\ntrain_df, temp_df = train_test_split(df, test_size=0.3, stratify=df[\"label_id\"], random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label_id\"], random_state=42)\n\nclass AudioDataset(Dataset):\n    def __init__(self, dataframe, sr=SR, n_samples=TARGET_SAMPLES, mode=\"train\"):\n        self.df = dataframe.reset_index(drop=True)\n        self.sr = sr\n        self.n_samples = n_samples\n        self.mode = mode\n        self.feature_extractor = Wav2Vec2FeatureExtractor(\n            feature_size=1, sampling_rate=sr, padding=True, do_normalize=True, return_attention_mask=True\n        )\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        x = load_audio(row[\"path\"], sr=self.sr)\n        x = rms_normalize(x)\n        crop_mode = \"random\" if self.mode == \"train\" else \"center\"\n        x = crop_or_pad(x, self.n_samples, mode=crop_mode)\n        if self.mode == \"train\":\n            x = train_augment(x)  # Fixed: removed sample_rate parameter\n\n        inputs = self.feature_extractor(x, sampling_rate=self.sr, padding=\"do_not_pad\")\n        input_values = torch.tensor(inputs[\"input_values\"], dtype=torch.float32)\n        attention_mask = torch.ones_like(input_values)\n\n        return {\n            \"input_values\": input_values.squeeze(0),\n            \"attention_mask\": attention_mask.squeeze(0),\n            \"labels\": torch.tensor(row[\"label_id\"], dtype=torch.long),\n            \"path\": row[\"path\"]\n        }\n\ntrain_ds = AudioDataset(train_df, mode=\"train\")\nval_ds   = AudioDataset(val_df,   mode=\"val\")\ntest_ds  = AudioDataset(test_df,  mode=\"test\")\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\ntest_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nlen(train_ds), len(val_ds), len(test_ds), labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model and training","metadata":{}},{"cell_type":"markdown","source":"- Base: facebook/wav2vec2-base\n- Head: New classification head with 3 outputs\n- Optimization: AdamW + linear scheduler\n- Class imbalance: Optional class weights\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\nfrom torch.utils.data import DataLoader\n\n# Setup\nMODEL_NAME = \"facebook/wav2vec2-base\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load processor and model\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=len(labels),\n    label2id=label2id,\n    id2label=id2label,\n    problem_type=\"single_label_classification\",\n    ignore_mismatched_sizes=True  # suppresses classifier/projector warnings\n)\nmodel.gradient_checkpointing_enable()\nmodel.to(DEVICE)\n\n# Freeze feature extractor for warmup\nfor param in model.wav2vec2.feature_extractor.parameters():\n    param.requires_grad = False\n\n# Optimizer and scheduler\n# Training schedule\nWARMUP_EPOCHS = 5   # only train classifier head\nFT_EPOCHS = 5       # fine-tune encoder + head\n\nLR = 2e-5\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nnum_training_steps = EPOCHS * len(train_loader)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_training_steps)\n\n# Class weights\nclass_counts = train_df[\"label_id\"].value_counts().sort_index().values\nclass_weights = torch.tensor((class_counts.max() / class_counts), dtype=torch.float32).to(DEVICE)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Training loop\ndef train_one_epoch(epoch):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_values = torch.tensor(np.array(batch[\"input_values\"]), dtype=torch.float32).to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n        labels = batch[\"labels\"].to(DEVICE)\n\n        outputs = model(input_values=input_values, attention_mask=attention_mask)\n        loss = criterion(outputs.logits, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item() * input_values.size(0)\n        preds = outputs.logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += input_values.size(0)\n\n    return total_loss / total, correct / total\n\n# Evaluation loop\n@torch.no_grad()\ndef eval_one_epoch(loader):\n    model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    all_labels, all_preds = [], []\n    for batch in loader:\n        input_values = torch.tensor(np.array(batch[\"input_values\"]), dtype=torch.float32).to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n        labels = batch[\"labels\"].to(DEVICE)\n\n        outputs = model(input_values=input_values, attention_mask=attention_mask)\n        loss = criterion(outputs.logits, labels)\n\n        total_loss += loss.item() * input_values.size(0)\n        preds = outputs.logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += input_values.size(0)\n\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(preds.cpu().numpy())\n\n    return total_loss / total, correct / total, np.array(all_labels), np.array(all_preds)\n\n# Training loop\ntrain_losses, val_losses, val_accs, train_accs = [], [], [], []\n\n\nfor epoch in range(1, WARMUP_EPOCHS + 1):\n    tr_loss, tr_acc = train_one_epoch(epoch)\n    val_loss, val_acc, y_true, y_pred = eval_one_epoch(val_loader)\n\n    train_losses.append(tr_loss)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    train_accs.append(tr_acc)\n    print(f\"[Warmup] Epoch {epoch:02d} | train_loss={tr_loss:.4f} acc={tr_acc:.3f} \"\n          f\"| val_loss={val_loss:.4f} acc={val_acc:.3f}\")\n\n\n# Unfreeze feature extractor for fine-tuning\nfor param in model.wav2vec2.feature_extractor.parameters():\n    param.requires_grad = True\n\n# Differential learning rates: smaller for encoder, larger for classifier\n\noptimizer = torch.optim.AdamW([\n    {\"params\": model.wav2vec2.parameters(), \"lr\": 1e-5},\n    {\"params\": model.classifier.parameters(), \"lr\": 2e-5},\n], weight_decay=0.01)\n\nfor epoch in range(WARMUP_EPOCHS + 1, WARMUP_EPOCHS + FT_EPOCHS + 1):\n    tr_loss, tr_acc = train_one_epoch(epoch)\n    val_loss, val_acc, y_true, y_pred = eval_one_epoch(val_loader)\n    train_losses.append(tr_loss)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    train_accs.append(tr_acc)\n    print(f\"[Warmup] Epoch {epoch:02d} | train_loss={tr_loss:.4f} acc={tr_acc:.3f} \"\n          f\"| val_loss={val_loss:.4f} acc={val_acc:.3f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation and analysis","metadata":{}},{"cell_type":"code","source":"#Training curves\nplt.figure(figsize=(8,4))\nplt.plot(train_losses, label=\"Train loss\")\nplt.plot(val_losses, label=\"Val loss\")\nplt.title(\"Loss curves\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(8,4))\nplt.plot(val_accs, label=\"Val accuracy\")\nplt.plot(train_accs, label=\"Train Accuracy\")\nplt.title(\"Validation accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test performance, confusion matrix, report\ntest_loss, test_acc, y_true_test, y_pred_test = eval_one_epoch(test_loader)\nprint(f\"Test loss={test_loss:.4f}, Test acc={test_acc:.3f}\")\n\ncm = confusion_matrix(y_true_test, y_pred_test, labels=list(range(len(labels))))\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=[id2label[i] for i in range(len(labels))],\n            yticklabels=[id2label[i] for i in range(len(labels))])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion matrix (test)\")\nplt.show()\n\nprint(classification_report(y_true_test, y_pred_test, target_names=[id2label[i] for i in range(len(labels))]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pydub\n\nfrom pydub import AudioSegment\nimport tempfile\nimport os\n\ndef convert_aac_to_wav(aac_path):\n    \"\"\"Convert AAC file to temporary WAV file\"\"\"\n    # Load AAC file\n    audio = AudioSegment.from_file(aac_path, format=\"aac\")\n    \n    # Create temporary WAV file\n    temp_wav = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n    audio.export(temp_wav.name, format=\"wav\")\n    return temp_wav.name\n\n@torch.no_grad()\ndef predict_file(path: str, n_crops=10):\n    model.eval()\n    \n    # Check if file is AAC and convert if needed\n    if path.lower().endswith('.aac'):\n        wav_path = convert_aac_to_wav(path)\n        try:\n            x = load_audio(wav_path, sr=SR)\n        finally:\n            # Clean up temporary file\n            os.unlink(wav_path)\n    else:\n        x = load_audio(path, sr=SR)\n        \n    x = rms_normalize(x)\n    # Rest of your function remains the same...\n    probs_list = []\n    for _ in range(n_crops):\n        crop = crop_or_pad(x, TARGET_SAMPLES, mode=\"random\")\n        inputs = Wav2Vec2FeatureExtractor(\n            feature_size=1, sampling_rate=SR, padding=True, do_normalize=True, return_attention_mask=True\n        )(crop, sampling_rate=SR, padding=\"do_not_pad\")\n        input_values = torch.tensor(inputs[\"input_values\"], dtype=torch.float32).to(DEVICE)\n        attention_mask = torch.ones_like(input_values)\n\n        outputs = model(input_values=input_values, attention_mask=attention_mask)\n        probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n        probs_list.append(probs)\n\n    mean_probs = np.mean(np.stack(probs_list, axis=0), axis=0)\n    pred_id = int(np.argmax(mean_probs))\n    return id2label[pred_id], mean_probs\n\n# Example usage:\npredicted_label, probabilities = predict_file(\"/kaggle/input/coco-sound/sounds dataset/MALAKANIN/malakanin10.wav\")\nprint(predicted_label, probabilities)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Export model","metadata":{}},{"cell_type":"code","source":"SAVE_DIR = Path(\"./wav2vec2_coconut_tap_model\")\nSAVE_DIR.mkdir(exist_ok=True)\nmodel.save_pretrained(SAVE_DIR)\nwith open(SAVE_DIR / \"label_map.json\", \"w\") as f:\n    json.dump({\"label2id\": label2id, \"id2label\": id2label}, f, indent=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\"wav2vec2_coconut_tap_model\", 'zip', SAVE_DIR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}